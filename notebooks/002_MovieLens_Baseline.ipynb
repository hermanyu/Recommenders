{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd2e121-da7e-4219-8958-be64f4c88b86",
   "metadata": {},
   "source": [
    "# MovieLens: A Basic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a2b72-4afc-40fe-8228-6ec9ec0b30c9",
   "metadata": {},
   "source": [
    "This notebook explores a basic regression model dubbed \"the baseline model\" to predict user-movie ratings on the MovieLens 25M dataset. We use the [Surprise Library](https://surprise.readthedocs.io/en/stable/index.html) to load and split data, as well as fit models. The theory behind the procedures follows [Koren, Y. - Factor in the Neighbors: Scalable and\n",
    "Accurate Collaborative Filtering](https://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5688e39f-27b8-4281-a333-7264bfd32181",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b62270-c05e-4769-aa09-67e68de8f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pdb\n",
    "import surprise\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "\n",
    "from source_code.Python import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6444f3e6-c458-463f-b067-a787927b575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = get_data.get_movielens_large_ratings()\n",
    "df_movies = get_data.get_movielens_large_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c837b3f0-0212-4444-a1bd-e86bf9944234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1      296     5.0  1147880044\n",
       "1       1      306     3.5  1147868817\n",
       "2       1      307     5.0  1147868828\n",
       "3       1      665     5.0  1147878820\n",
       "4       1      899     3.5  1147868510"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7aacb65-486b-4e11-a9c7-7ccd3183d298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId         int64\n",
       "movieId        int64\n",
       "rating       float64\n",
       "timestamp      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8b565-2dab-4b82-a750-9a9f887a9ed6",
   "metadata": {},
   "source": [
    "# The Surprise Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ec459-50ca-4278-a53f-d3b41ada573c",
   "metadata": {},
   "source": [
    "The `surprise` library is a scikit learn style package that allows for constructions of well-known recommender systems. For this notebook, we will be exploring a basic linear regression-style model to predict user-movie ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a904c7-7d70-4638-9a59-b7430e3e42c0",
   "metadata": {},
   "source": [
    "## Readers and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbbf1e-8739-4330-afba-1c62f320abb4",
   "metadata": {},
   "source": [
    "In order to use the built-in functions of `surprise`, we must first construct a framework for how to prepare and ingest our data set. The two main components of the `surprise` library that helps us do this are:\n",
    "\n",
    "1) `Reader` objects. These object store metadata for how to interpret our data set.\n",
    "2) The `Dataset` module which contains functions that load/stream data from the source. The source can either be a file on a disk (e.g. a CSV file) or a pandas dataframe.\n",
    "\n",
    "The idea is to stream rows of data using the `Dataset` functions into memory, while using the `Reader` object to understand that data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3aaff08-d165-45ae-8cfc-be6005f2a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reader and Dataset for a csv file;\n",
    "# Loading data from a file requires the load_from_file() method;\n",
    "# NOTE: if the csv has a header (ie column names) we have to use the \"skip_lines\" parameter\n",
    "file_path = \"../data/movielens/large/ratings.csv\"\n",
    "\n",
    "reader_csv = surprise.Reader(line_format = \"user item rating timestamp\", sep = \",\", skip_lines = 1)\n",
    "dataset_csv = surprise.Dataset.load_from_file(file_path, reader = reader_csv)\n",
    "\n",
    "# Reader and Dataset on an existing pandas dataframe;\n",
    "# loading data from a dataframe requires the load_from_df() method\n",
    "reader_df = surprise.Reader(rating_scale = (1, 5))\n",
    "dataset_df = surprise.Dataset.load_from_df(df_ratings[[\"userId\", \"movieId\", \"rating\"]], reader = reader_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f8465-da73-4246-ab0a-c7906a4b4a1c",
   "metadata": {},
   "source": [
    "# Baseline Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a90cfa5-ba0b-4d5b-bbac-cc65774987ae",
   "metadata": {},
   "source": [
    "Let us begin from the ground up by asking the simple question: \"What do we think user $u$ will rate movie $j$?\" \n",
    "\n",
    "If the user has already rated the movie, than no prediction is necessary on other part. The challenge comes for movies that user $i$ has not yet rated. For such missing ratings, we have to impute the values and thus the problem is fundamentally a problem of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c2a26-b6e4-4e9b-b5dd-05a326283a9e",
   "metadata": {},
   "source": [
    "## Ratings Data: Pooling All Users and Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661131f-cd8c-4f42-b4e9-fd4e0a9ae59a",
   "metadata": {},
   "source": [
    "Let's start at the most atomic level and suppose all we had was a list `ratings = [1.0, 3.5, 2.5, ...]`. In other words, we treat all users and all movies as the same and think about a giant pool of ratings. \n",
    "\n",
    "The most simple approach is to use the **average** across all movies by all users. Recall that the (arithmetic) average $\\mu$ is the \"constant of best fit\" in the sense that $\\mu$ is the constant which minimizes the mean squared error. Absent any other information, this numerical value $\\mu$ would be our best-guess at what a missing rating is most likely to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bfa9608-ea04-4be8-81db-d7fdce4ea82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating    3.533854\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_bar = df_ratings[['rating']].mean()\n",
    "\n",
    "r_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d816e-d2cc-464f-a162-f65b39c080d1",
   "metadata": {},
   "source": [
    "In the MovieLens data set, our $\\mu$ has a value of 3.53, so this gives us an initial guess at what the missing ratings might be.\n",
    "\n",
    "$$r_{uj} := \\mu = 3.53$$\n",
    "\n",
    "for all users $u$ and movies $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec843fae-52cc-4d08-bf54-332bbd90e277",
   "metadata": {},
   "source": [
    "## Item Bias: Accounting For Good and Bad Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b725917-a1c9-43a0-9660-1227d8eebc98",
   "metadata": {},
   "source": [
    "The simplistic approach above only usings the `ratings` data but not the movie ID data. Specifically, the simplistic approach ignores the fact that the ratings belong to different movies. This is problematic because the movie ratings belong to clusters based on the movie being rated. For example:\n",
    "\n",
    "- All the ratings with `movieId` equal to 1 belong to Toy Story, which is generally regarded as a very good film. Therefore, we would expect all ratings with `movieId == 1` to generally be higher than an average movie. \n",
    "- Conversely, all the ratings with `movieId` equal to 74754 belong to The Room (starring Tommy Wiseau), which is generally regarded as a terrible film. Therefore, we would expect all such ratings to be generally lower than an average movie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a737f1c-15e3-4f95-802a-767c4caaa358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14391</th>\n",
       "      <td>74754</td>\n",
       "      <td>Room, The (2003)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId             title                                       genres\n",
       "0            1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
       "14391    74754  Room, The (2003)                         Comedy|Drama|Romance"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies[(df_movies[\"movieId\"] == 1) | (df_movies[\"movieId\"] == 74754)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99e38313-2ada-4dff-a1a9-294f5348ee5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating    3.893708\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average rating for Toy Story\n",
    "df_ratings[df_ratings[\"movieId\"] == 1][[\"rating\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ab41a39-37a3-46e7-96d2-8cae5825ef89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating    2.403731\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average rating for The Room\n",
    "df_ratings[df_ratings[\"movieId\"] == 74754][[\"rating\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c52fc-d622-41d3-b8de-ad2376629485",
   "metadata": {},
   "source": [
    "As expected:\n",
    "\n",
    "- Toy Story has an average rating of 3.89 which is +0.36 higher than the global average of 3.53\n",
    "- The Room has an average rating of 2.40 which is -1.13 lower than the global average of 3.53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690bdcc-7a6a-4077-ae4b-c571218a1be0",
   "metadata": {},
   "source": [
    "So how do we incorprate this information into our prediction? What we can do is take our simplistic global average of 3.53 and adjust it based on how much each individual movie is above or below the global average.\n",
    "\n",
    "$$r_{uj} := \\mu + b_j = 3.53 + b_j$$\n",
    "\n",
    "where $b_j$ is the difference between movie $j$'s average rating and the global average $\\mu$. Note that this is equivalent to saying $r_{uj} = \\text{average rating for movie }j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c0412-8f5b-406b-a2c6-3357262786a7",
   "metadata": {},
   "source": [
    "So for example, if we wanted to predict user $u$'s rating for Toy Story, it would just be:\n",
    "\n",
    "$$r_{u1} = \\mu + b_1 = 3.53 + 0.36 = 3.89$$\n",
    "\n",
    "Similarly, if we wanted to predict user $u$'s rating for The Room, it would just be:\n",
    "\n",
    "$$r_{u,74754} = \\mu + b_{74754} = 3.53 - 1.13 = 2.40$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bc392-239f-4f22-89e1-6309af73a986",
   "metadata": {},
   "source": [
    "## User Bias: Accounting for Harsh and Lenient Reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb54606-9829-41a2-bd9e-435349ffef09",
   "metadata": {},
   "source": [
    "In the same way that movies can be above or below average, a user can be more harsh or lenient when it comes to movie reviews. Some users generally rate movies loweer than the average person, while some users generally rate movies higher than the average person. A good way to think about this is to recognize that different people have different grading scales: some people think that a 3-star rating represents an average movie, while other people think that a 3-star rating represents a below average movie.\n",
    "\n",
    "We can account for this type of information in exactly the same way as before: by considering the average value across all ratings given out by a user.\n",
    "\n",
    "$$r_{uj} = \\mu + b_u + b_j = 3.53 + b_u + b_j$$\n",
    "\n",
    "where $b_u$ is the difference between user $u$'s average rating and the global average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab3667-26c2-41d3-9ecc-bd29aaf5cdb6",
   "metadata": {},
   "source": [
    "## A Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7405f367-abb0-4d47-be18-dad62b624202",
   "metadata": {},
   "source": [
    "We can unify all 3 considerations together and derive a baseline model that predicts user ratings:\n",
    "\n",
    "$$\\hat{r}_{uj} = \\mu + b_u + b_j$$\n",
    "\n",
    "In the discussion above, the parameters $b_u$ and $b_j$ are known by taking the averages of the ratings by user and by movie.\n",
    "\n",
    "A more sophisticated approach is to treat this as a statistical model by allowing for sampling variance in the data set. More precisely, $b_u$ should represent a user $u$'s average rating across *all* movies in the data set. Similarly, $b_j$ should represent movie $j$'s average rating across *every* person in the data set.\n",
    "\n",
    "From this perspective, the movie ratings in our data set are just a sample from larger population; we only observe a few of the possible user-movie ratings. The values of $\\mu$, $b_u$, and $b_j$ will almost certainly fluctuate if we had observed a different combination of user-movie ratings. This fluctation is *sampling variance* and we want to account for this uncertainty in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89fcf1-d78d-4624-920c-3f1ca0b015d2",
   "metadata": {},
   "source": [
    "We can thus approach this as a linear regression problem. Our hypothesized probability model is:\n",
    "\n",
    "$$r_{uj} = \\mu + \\sum_{u = 1}^{U} b_i\\cdot I(i) + \\sum_{j = 1}^{M} b_j \\cdot I(j) + \\epsilon$$\n",
    "\n",
    "\n",
    "where $I(i)$ and $I(j)$ are indicator variables for user $i$ and movie $j$ and $\\epsilon \\sim N(0, \\sigma)$. From this probability model, it is possible to calculate a *likelihood* for each possible value of $\\mu$, $b_i$, and $b_j$ based on the data we are currently observing:\n",
    "\n",
    "$$\\mathcal{L}(\\mu, b_i, b_j | X) = Pr(X | \\mu, b_i, b_j)$$\n",
    "\n",
    "Our \"best guess\" estimates for $\\mu$, $b_i$, are $b_j$ the values of $\\mu$, $b_i$, $b_j$ which maximizise this likelihood:\n",
    "\n",
    "$$\\mu, b_i, b_j = \\max_{\\mu, b_i, b_j} \\mathcal{L}(\\mu, b_i, b_j | X)$$\n",
    "\n",
    "As it turns out, the solution to this maximization problem is precisely the same as the solution to that minimizes the following loss function:\n",
    "\n",
    "$$min \\sum_{(u, j) \\in R} (r_{uj} - \\mu - b_u - b_i)^2$$\n",
    "\n",
    "Finally, we can also stipulate some Bayesian priors $b_i \\sim N(0, \\lambda_1)$ and $b_j \\sim N(0, \\lambda_2)$ and the result is the following minimization problem:\n",
    "\n",
    "$$min \\sum_{(u, j) \\in R} (r_{uj} - \\mu - b_u - b_i)^2 + \\lambda_1\\sum_{i} b_i^2 + \\lambda_2\\sum_{j} b_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e9474-c50a-4b63-8d7e-722ac4e7a6a0",
   "metadata": {},
   "source": [
    "## Baseline Model with Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194473a9-0550-4847-b082-23e1f6bee4c3",
   "metadata": {},
   "source": [
    "The ideas discussed above can be achieved using the `BaselineOnly` model in `surprise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5848b3f-78db-4144-9daa-706bb53de8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Evaluating RMSE, MAE of algorithm BaselineOnly on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8602  0.8594  0.8597  0.8599  0.8597  0.8598  0.0003  \n",
      "MAE (testset)     0.6566  0.6561  0.6563  0.6563  0.6562  0.6563  0.0002  \n",
      "Fit time          86.41   93.15   93.18   95.98   94.42   92.63   3.28    \n",
      "Test time         47.17   46.57   40.85   48.71   41.37   44.94   3.20    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.86019934, 0.85942653, 0.8596957 , 0.85988404, 0.85967176]),\n",
       " 'test_mae': array([0.65660484, 0.65611804, 0.65627582, 0.65629546, 0.65619791]),\n",
       " 'fit_time': (86.40905952453613,\n",
       "  93.15286207199097,\n",
       "  93.17840123176575,\n",
       "  95.97624039649963,\n",
       "  94.42246890068054),\n",
       " 'test_time': (47.1684045791626,\n",
       "  46.573326587677,\n",
       "  40.85033297538757,\n",
       "  48.71293568611145,\n",
       "  41.37364172935486)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a Baseline Model\n",
    "baseline_model = surprise.BaselineOnly()\n",
    "\n",
    "# perform 5-fold cross validation to evaluate model specification\n",
    "surprise.model_selection.cross_validate(baseline_model, dataset_df, measures = [\"RMSE\", \"MAE\"], cv = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e952680-b5eb-4c34-ade0-d61f40b6a310",
   "metadata": {},
   "source": [
    "The baseline model has a mean MSE of 0.86 across the 5 folds with a mean MAE of 0.66. This means on average, the baseline model deviates by more little more than half-a-star.\n",
    "\n",
    "Notice in the console output, the biases were estimated using ALS or **Alternating Least Squares**. This procedure starts by randomly assigning values to $b_i$ and minimizing with respect to $b_u$ first, while holding $b_i$ fixed. Once the $b_u$ are estimated, the procedure minimizes with respect to $b_i$ holding $b_u$ fixed. Then we return and re-minize to $b_u$ and so on, alternating between minimizing one the sets $b_i$ and $b_u$, while holding the other fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53af9a-2454-4082-b7a2-86b38a4cf76f",
   "metadata": {},
   "source": [
    "We can also do a more rudimentary train-test split: fit the model on the training set, and evaluate the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f976226e-e08f-4e64-b99a-1073bb8955af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split with 30% going to the test set\n",
    "trainset, testset = surprise.model_selection.train_test_split(dataset_df, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d23ecd74-c1c1-41ff-bde9-1f9f824bee06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.baseline_only.BaselineOnly at 0x25609d80e80>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit baseline model on training set\n",
    "baseline_model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e484d48f-e08b-41c5-a102-c3663c39710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8602156904794773"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on test set and evaluate performance\n",
    "predictions = baseline_model.test(testset)\n",
    "\n",
    "surprise.accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb3beaa1-496a-4591-8b83-91122a0d818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1          item: 74754      r_ui = None   est = 3.53   {'was_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(uid='1', iid='74754', r_ui=None, est=3.5335277878380573, details={'was_impossible': False})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a prediction on a user-movie rating\n",
    "# note that userId and movieId must be passed as a string\n",
    "baseline_model.predict(uid = \"1\", iid = \"74754\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662f74a-db84-4662-86c8-efa091eb80f4",
   "metadata": {},
   "source": [
    "The baseline model predicts that User 1 will give Movie 74754 (The Room) a rating of 3.5, implying that User 1 would rate The Room higher than an average user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adfe9202-e3e5-4172-b5f3-652efd40fc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId       1.000000e+00\n",
       "movieId      6.740814e+03\n",
       "rating       3.814286e+00\n",
       "timestamp    1.147873e+09\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check User 1's average rating given out\n",
    "df_ratings[df_ratings[\"userId\"] == 1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c9d59-1192-463b-bbd0-613b618a5550",
   "metadata": {},
   "source": [
    "We see User 1 generally gives out an average rating of 3.8, so a rating of 3.5 is actually below average for User 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0363bf8-9fd8-4861-af5c-cf29802e3aaf",
   "metadata": {},
   "source": [
    "We can also play around with hyperparameters and options for the baseline model; this is done by passing a dictionary. Note that the regularization parameters were chosen based on [Koren 2010] which mentions that `reg_u = 25` and `reg_i = 10` were found to work the best on the Netflix Prize dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe593c87-e1a7-47ca-afc5-cf97d58e2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters and estimation method\n",
    "baseline_options = {\n",
    "    \"method\": \"als\",  # estimation method\n",
    "    \"n_epochs\": 5,    # number of iterations of ALS procedure; default is 10\n",
    "    \"reg_u\": 25,      # regularization parameter for user biases\n",
    "    \"reg_i\": 10       # regularization parameter for movie biases\n",
    "}\n",
    "\n",
    "baseline_model = surprise.BaselineOnly(bsl_options = baseline_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81536c1c-f841-4a49-9efc-1760d438c51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 0.8622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.862169563962609"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and test\n",
    "baseline_model.fit(trainset)\n",
    "\n",
    "predictions = baseline_model.test(testset)\n",
    "\n",
    "surprise.accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440cfe93-c66e-45ed-9b3e-ce50b4b84568",
   "metadata": {},
   "source": [
    "Since we have hyperparameters we can tune, we can further attempt to optimize the model by using a grid search to select optimal hyperparameters. Actual training time will become rather long on the full 25 million row dataset, so we switch over to the 100,000 row dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75812ecd-c272-4ed9-85be-5bf16606e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "0.8763149214791635\n",
      "0.6758180562256308\n",
      "{'bsl_options': {'reg_u': 10, 'reg_i': 10}}\n"
     ]
    }
   ],
   "source": [
    "# load smaller dataset\n",
    "df_ratings = get_data.get_movielens_small_ratings()\n",
    "\n",
    "# setup Reader obejct and Dataset loader\n",
    "reader_df = surprise.Reader(rating_scale = (1, 5))\n",
    "dataset_df = surprise.Dataset.load_from_df(df_ratings[[\"userId\", \"movieId\", \"rating\"]], reader = reader_df)\n",
    "\n",
    "# setup param_grid that we search over\n",
    "# Note: for the BaselineOnly model, the hyperparameters are passed\n",
    "# as a dictionary to the bsl_options argument. \n",
    "# This means our param_grid will have to be finessed like so:\n",
    "param_grid = {\n",
    "    \"bsl_options\" : {\n",
    "        \"reg_u\" : [10, 25],\n",
    "        \"reg_i\" : [10, 25]\n",
    "    }\n",
    "}\n",
    "\n",
    "# instantiate GridSearchCV object\n",
    "gs = surprise.model_selection.GridSearchCV(\n",
    "    surprise.BaselineOnly, \n",
    "    param_grid, \n",
    "    measures = [\"rmse\", \"mae\"], \n",
    "    cv = 3\n",
    ")\n",
    "\n",
    "# conduct grid search\n",
    "gs.fit(dataset_df)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score[\"rmse\"])\n",
    "\n",
    "# best MAE score\n",
    "print(gs.best_score[\"mae\"])\n",
    "\n",
    "# return the best configuration wrt to rmse\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33004765-21d5-48e0-9b3c-044a59046d91",
   "metadata": {},
   "source": [
    "# Limitations and Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe4356-761f-431f-9742-2ba618e117b1",
   "metadata": {},
   "source": [
    "The baseline model is simplistic and a \"natural\" first choice, but it's simplicity comes at the cost of limitations.\n",
    "\n",
    "- The model is only able to make predictions on users and movies that exist in the training set. In particular, if a new user joins and rates a few movies, we would need to refit in order to make predictions on this user's ratings.\n",
    "- More generally, if a new user joins the data set, the model cannot make predictions on their ratings *until* we learn about the new user's preferences. This is a fundamental and universal problem in recommender systems called the **cold start problem**.\n",
    "- Our model does not take into account similarities between users' rating behaviors. Specifically, if User 1 and User 2 both rate a common set of movies very highly, it is likely that User 1's ratings can be used to predict User 2's ratings (and vice versa). This idea of using common rating behaviors across users is called **user-based collaborative filtering**.\n",
    "- Similarly, our model does not take into account similarities between movies' rating behaviors. If Movie 1 and Movie 2 are both rated highly by a common group of users, then it is likely that other movies rated highly by a user in the group will be rated highly by all users in the group. This idea of using common rating patterns across movies is called **item-baesd collaborative filtering**.\n",
    "- Our model does not incorporate any additional pieces of information about the movies such as genre, tags, and text-based reviews. For example, if User 1 rated The Terminator, The Matrix, and Star Wars very highly, then it is likely User 1 will rate Sci-Fi Action movies very highly in general. This idea of using implicit characteristics of movies is called **content-based recommendation**.\n",
    "- Additionally, the model cannot learn to recognize which movies are similar. If a group of users rates The Terminator and The Matrix very similarly, then we should be able to infer that these two movies must have *something* in common (i.e. they have the same \"vibe\") This idea of finding implicit characteristics between movies is called **latent factor modeling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c81a5d-1b96-4907-88a0-d77af35bd242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
